[
  {
    "objectID": "MV_HS_Visit_S2025.html#definition",
    "href": "MV_HS_Visit_S2025.html#definition",
    "title": "Data Science",
    "section": "Definition:",
    "text": "Definition:\nIBM: “Data science combines math and statistics, specialized programming, advanced analytics, artificial intelligence (AI) and machine learning with specific subject matter expertise to uncover actionable insights hidden in an organization’s data. These insights can be used to guide decision making and strategic planning.”"
  },
  {
    "objectID": "MV_HS_Visit_S2025.html#meet-data-science",
    "href": "MV_HS_Visit_S2025.html#meet-data-science",
    "title": "Data Science",
    "section": "Meet data science",
    "text": "Meet data science\n\nData science is an exciting discipline that allows you to turn raw data into understanding, insight, and knowledge."
  },
  {
    "objectID": "MV_HS_Visit_S2025.html#r",
    "href": "MV_HS_Visit_S2025.html#r",
    "title": "Data Science",
    "section": "R",
    "text": "R"
  },
  {
    "objectID": "MV_HS_Visit_S2025.html#rstudio",
    "href": "MV_HS_Visit_S2025.html#rstudio",
    "title": "Data Science",
    "section": "RStudio",
    "text": "RStudio"
  },
  {
    "objectID": "MV_HS_Visit_S2025.html#data-science-life-cycle-1",
    "href": "MV_HS_Visit_S2025.html#data-science-life-cycle-1",
    "title": "Data Science",
    "section": "Data science life cycle",
    "text": "Data science life cycle"
  },
  {
    "objectID": "MV_HS_Visit_S2025.html#import",
    "href": "MV_HS_Visit_S2025.html#import",
    "title": "Data Science",
    "section": "Import",
    "text": "Import"
  },
  {
    "objectID": "MV_HS_Visit_S2025.html#tidy-transform",
    "href": "MV_HS_Visit_S2025.html#tidy-transform",
    "title": "Data Science",
    "section": "Tidy + transform",
    "text": "Tidy + transform"
  },
  {
    "objectID": "MV_HS_Visit_S2025.html#visualize",
    "href": "MV_HS_Visit_S2025.html#visualize",
    "title": "Data Science",
    "section": "Visualize",
    "text": "Visualize"
  },
  {
    "objectID": "MV_HS_Visit_S2025.html#model",
    "href": "MV_HS_Visit_S2025.html#model",
    "title": "Data Science",
    "section": "Model",
    "text": "Model"
  },
  {
    "objectID": "MV_HS_Visit_S2025.html#understand",
    "href": "MV_HS_Visit_S2025.html#understand",
    "title": "Data Science",
    "section": "Understand",
    "text": "Understand"
  },
  {
    "objectID": "MV_HS_Visit_S2025.html#section",
    "href": "MV_HS_Visit_S2025.html#section",
    "title": "Data Science",
    "section": "",
    "text": "# A tibble: 5 × 2\n  date             season\n  &lt;chr&gt;            &lt;chr&gt; \n1 23 January 2017  winter\n2 4 March 2017     spring\n3 14 June 2017     summer\n4 1 September 2017 fall  \n5 ...              ..."
  },
  {
    "objectID": "MV_HS_Visit_S2025.html#communicate",
    "href": "MV_HS_Visit_S2025.html#communicate",
    "title": "Data Science",
    "section": "Communicate",
    "text": "Communicate"
  },
  {
    "objectID": "MV_HS_Visit_S2025.html#understand-communicate",
    "href": "MV_HS_Visit_S2025.html#understand-communicate",
    "title": "Data Science",
    "section": "Understand + communicate",
    "text": "Understand + communicate"
  },
  {
    "objectID": "MV_HS_Visit_S2025.html#program",
    "href": "MV_HS_Visit_S2025.html#program",
    "title": "Data Science",
    "section": "Program",
    "text": "Program"
  },
  {
    "objectID": "MV_HS_Visit_S2025.html#what-you-might-actually-be-doing",
    "href": "MV_HS_Visit_S2025.html#what-you-might-actually-be-doing",
    "title": "Data Science",
    "section": "What you might actually be doing?",
    "text": "What you might actually be doing?\n\nBusiness intelligence\nCybersecurity analysis\nData visualization\n\n\n\nhttps://www.indeed.com/career-advice/finding-a-job/types-of-data-science-jobs"
  },
  {
    "objectID": "MV_HS_Visit_S2025.html#sure-but-what-are-you-doing",
    "href": "MV_HS_Visit_S2025.html#sure-but-what-are-you-doing",
    "title": "Data Science",
    "section": "Sure, but what are you doing",
    "text": "Sure, but what are you doing\n\nDemand prediction for the manufacturing industry\nRecommendation systems in marketing & advertising\nCredit scoring for financial institutions\n\n\n\nhttps://addepto.com/data-science-examples-see-real-applications/"
  },
  {
    "objectID": "MV_HS_Visit_S2025.html#job-growth",
    "href": "MV_HS_Visit_S2025.html#job-growth",
    "title": "Data Science",
    "section": "Job Growth",
    "text": "Job Growth\nThe US Department of Labor Statistics continues to project growth for Data Science\nhttps://www.bls.gov/ooh/math/data-scientists.htm"
  },
  {
    "objectID": "MV_HS_Visit_S2025.html#courses",
    "href": "MV_HS_Visit_S2025.html#courses",
    "title": "Data Science",
    "section": "Courses",
    "text": "Courses\n\nStatistics Core\nIntroduction to Data Science\nComputer Science\nStatistical and Machine Learning"
  },
  {
    "objectID": "MV_HS_Visit_S2025.html#competitions",
    "href": "MV_HS_Visit_S2025.html#competitions",
    "title": "Data Science",
    "section": "Competitions",
    "text": "Competitions\n\nMidwest Undergraduate Data Analytics Competition (MUDAC)\n\nA marathon\n\nMinneMUDAC\nBoth\n\nMeet industry professionals\nPresent your work to faculty and data scientists at companies\nPractice on real world applications"
  },
  {
    "objectID": "MV_HS_Visit_S2025.html#research",
    "href": "MV_HS_Visit_S2025.html#research",
    "title": "Data Science",
    "section": "Research",
    "text": "Research\n\nCornell Summer Research Institute\nExternal Opportunities"
  },
  {
    "objectID": "MV_HS_Visit_S2025.html#activity-intro",
    "href": "MV_HS_Visit_S2025.html#activity-intro",
    "title": "Data Science",
    "section": "Activity Intro",
    "text": "Activity Intro"
  },
  {
    "objectID": "MV_HS_Visit_S2025.html#what-have-you-been-listening-too",
    "href": "MV_HS_Visit_S2025.html#what-have-you-been-listening-too",
    "title": "Data Science",
    "section": "What have you been listening too?",
    "text": "What have you been listening too?\nOn your phone, answer the Google Survey at\nbit.ly/MV_favs or scan the QR code."
  },
  {
    "objectID": "lab-16-tidy-text.html",
    "href": "lab-16-tidy-text.html",
    "title": "Text Analysis in R",
    "section": "",
    "text": "Text analysis\nSentiment analysis\nBing vs Afinn Sentiment Lexicons\n\n\n\nIn addition to tidyverse we will be using other packages today\n\n\n\n\nUsing tidy data principles can make many text mining tasks easier, more effective, and consistent with tools already in wide use.\nLearn more at https://www.tidytextmining.com/.\n\n\n\n\n\n\n\nConsider some of the lyrics from There Is a Light That Never Goes Out by The Smiths\n\ntext &lt;- c(\"Take me out tonight\",\n          \"Where there's music and there's people\",\n          \"And they're young and alive\",\n          \"Driving in your car\",\n          \"I never never want to go home\",\n          \"Because I haven't got one\",\n          \"Anymore\")\ntext\n\nThis form of the text is not very easy to do analysis with. Think about the data you have been working with this week.\n\n\nQUESTION: How should data be formatted? ##################################\n\n\n\nThis will clean (tidy) the data!\n\ntext_df &lt;- tibble(line = 1:7, text = text)\n\ntext_df\n\nThis is still not quite right. Today, we want a word on each line. This process is called tokenizing, where we separate words from their sentences,.\n\ntext_df |&gt;\n  unnest_tokens(word, text)\n\n\n\n\nI am going to have R read in all of your results from the survey before.\n\n#listening &lt;- read_csv(\"data/listening.csv\")\n\nlistening &lt;- \n  read_sheet(\"https://docs.google.com/spreadsheets/d/1WXKCB_a8AO3CFp49VQ5yTSHYpOfDm1WA9mmYN5jTH1M/edit?usp=sharing\") |&gt; \n  janitor::clean_names() |&gt; # re-names the column to be called \"songs\"\n  select(songs) # keeps only the column called songs\n  \nlistening\n\n\n\n\n\nlistening |&gt;\n  unnest_tokens(word, songs) |&gt;\n  count(word, sort = TRUE)\n\n\n\n\n\nword_counts&lt;-\n  listening |&gt;\n  unnest_tokens(word, songs) |&gt;\n  anti_join(stop_words) |&gt;                           \n  count(word, sort = TRUE)\n\nword_counts\n\n\n\n\n\nword_counts |&gt;\n  ggplot(aes(x = fct_reorder(word, n), y = n)) +\n  geom_col() +\n  labs(x = \"Common words\", y = \"Count\") +\n  coord_flip()\n\n\n\n\n\nset.seed(1234)\nwordcloud(words = top20_songs$word, \n          freq = top20_songs$n, \n          colors = brewer.pal(5,\"Blues\"),\n          random.order = FALSE)\n#Note: You may need to increase the size of your plot area to get this to display properly\n\n\n\n\n\nLet’s look at a whole book, The Road to Oz by L. Frank Baum.\nWe will use an open source website of books called Project Gutenberg.\n\nfull_text &lt;- gutenberg_download(26624) # The number is the ebook number for the book on the website. \n\nNow it’s time to tokenize, where we separate words from their sentences, and tidy were R puts each word on its own line in a dataset.\n\ntidy_book &lt;- full_text |&gt; # sends the full book \n  mutate(line = row_number()) |&gt; # give s row number to each line\n  unnest_tokens(word,text) # separates the words \"tokens\" onto their own lines\n  # This stands for un - nest - tokens\n\ntidy_book\n\n\n\n\ntidy_book |&gt;\n  count(word,sort = T) # this counts and sorts by top counts\n\n\n\n\nNow we want a visualization to compare the common words\n\n  tidy_book |&gt;\n  count(word) |&gt;\n  slice_max(n, n = 20) |&gt;\n  ggplot(aes(n, fct_reorder(word, n))) +  \n    geom_col()\n\n\n\nQUESTION: Why is this not particularly useful?\nDiscuss with your group mates a step we could take before making this graph that would lead to a better, more informative, visualization. ################################## ANSWER:\n\n\n\n\n\n\n\n\nIn computing, stop words are words which are filtered out before or after processing of text.\nThey usually refer to the most common words in a language, but there is not a single list of stop words used by all natural language processing tools.\n\n\n\n\n\nget_stopwords()\n\n\n\n\n\nget_stopwords(language = \"es\")\n\n\n\n\n\n  tidy_book |&gt;\n  anti_join(get_stopwords(source = \"smart\")) |&gt; # This keeps only non-stopwords\n  count(word, sort = TRUE) |&gt; #count the frequency of the remaining words\n  slice_max(n, n = 20) |&gt; # keep only the top 20 counts\n  ggplot(aes(n, fct_reorder(word, n))) +  #start a blank graph\n    geom_col() #add a column (bar) graph to the blank graph\n\n\n\n\nOne way to think about text is the sentiment of words or phrases. These are values that represent positive and negative words and phrases. For example, if someone died, that is more negative than say, if someone got a promotion.\nExplore some sentiment lexicons, or collections of words and phrases with assigned sentiments.\n\nget_sentiments(\"bing\")\n\n\nget_sentiments(\"afinn\")\n\n\n\nQUESTION: How are these two lexicons of sentiment different? ################################## ANSWER:\n\n\n\n\n\n\n\nWe want a sentiment for each word but only for the words in our book.\n\ntidy_book\n\ntidy_book %&gt;%\n  inner_join(get_sentiments(\"bing\")) |&gt; # keeps only words with sentiments in bing\n  count(sentiment, sort = TRUE)\n\n\n\nQUESTION: What does your group think the “inner join” does? ################################## ANSWER:\n\n\n\n\n\n\nQUESTION: Consider the final result. What does this output tell you about the book? ################################## ANSWER:\n\n\n\n\n\n\nQUESTION: This is very broad information, what else do you want to know about the sentiment of the text? How could we change out approach (not code) to get more informative/useful results? ################################## ANSWER:\n\n\n\n\n\n\n\nLet’s consider the afinn lexicon now.\n\ntidy_book |&gt;\n  inner_join(get_sentiments(\"afinn\")) |&gt; # get sentiment of each word\n  summarize(sum(value))\n\n\n\nQUESTION: When we used bing lexicon above we counted the number of positive words and the number of negative words.\nThis new code adds up the value of all the words assigned sentiments.\nFor example, the word happy has a sentiment of 3, and sad has a sentiment of -2, and if we add them, 3 + (-2) we get an overall sentiment of 1.\nCompare and contrast the application of afinn and bing:\n\nWhich gives more information about your book?\nWhich is lexicon is more likely to be subjective? Why?\n\n\n\n\nANSWER:\n\n\n\n\n\n\n\n\n\n\nThe top 10 positive words:\n\ntidy_book |&gt;\n  inner_join(get_sentiments(\"afinn\")) |&gt; # get sentiment of each word\n  group_by(word) |&gt;\n  summarize(sum_sentiment = sum(value))|&gt; \n  slice_max(sum_sentiment,n=10)\n\nThis code\n\nfinds the sentiment for each word (such as happy has a sentiment of 3),\ntells R to sum across each word. If happy was in the text 3 times, it would do 3+3+3 for happy and give it a score of 9.\noutputs the top 10 words that have contributed the most sentiment in the entire book\n\nSimilarly the top 10 most negative words.\n\ntidy_book |&gt;\n  inner_join(get_sentiments(\"afinn\")) |&gt; # get sentiment of each word\n  group_by(word) |&gt;\n  summarize(sum_sentiment = sum(value))|&gt; \n  slice_min(sum_sentiment,n=10)\n\n\n\n\n\nset.seed(1234)\n\ntop_positives &lt;- \n      tidy_book |&gt;\n      inner_join(get_sentiments(\"afinn\")) |&gt; # get sentiment of each word\n      group_by(word) |&gt;\n      summarize(sum_sentiment = sum(value))|&gt; \n      slice_max(sum_sentiment,n=50) \n\nwordcloud(words = top_positives$word, \n        freq = top_positives$sum_sentiment, \n        colors = brewer.pal(5,\"Blues\"),\n        random.order = FALSE)\n\n\n\n\n\ntop_negatives &lt;- \n      tidy_book |&gt;\n      inner_join(get_sentiments(\"afinn\")) |&gt; # get sentiment of each word\n      group_by(word) |&gt;\n      summarize(sum_sentiment = sum(value))|&gt; \n      slice_min(sum_sentiment,n=50) \n\nwordcloud(words = top_negatives$word, \n        freq = abs(top_negatives$sum_sentiment), \n        colors = brewer.pal(5,\"Reds\"),\n        random.order = FALSE)\n\n\n\n\n\nWhat book does your table want to analyze today?\n\nGo to https://www.gutenberg.org/browse/scores/top\nClick on a top 100 book or search a book you are interested in. Not all are available and not all will be downloadable.\nReplace 26624 below with your own choice. You need the EBook-No. from the website. It can be found by scrolling down in the table on the page for the book or in the URL.\n\n\nfull_text &lt;- gutenberg_download(26624) # The number is the ebook number for the book on the website. \n\n\n\n\ntidy_book &lt;- full_text |&gt; # sends the full book \n  mutate(line = row_number()) |&gt; # give s row number to each line\n  unnest_tokens(word,text) # separates the words \"tokens\" onto their own lines\n  # This stands for un - nest - tokens\n\ntidy_book\n\n\n\n\n\n  tidy_book |&gt;\n  count(word) |&gt;\n  slice_max(n, n = 20) |&gt;\n  ggplot(aes(n, fct_reorder(word, n))) +  \n    geom_col()\n\n\n\n\n\n  tidy_book |&gt;\n  anti_join(get_stopwords(source = \"smart\")) |&gt; # This keeps only non-stopwords\n  count(word, sort = TRUE) |&gt; #count the frequency of the remaining words\n  slice_max(n, n = 20) |&gt; # keep only the top 20 counts\n  ggplot(aes(n, fct_reorder(word, n))) +  #start a blank graph\n    geom_col() #add a column (bar) graph to the blank graph\n\n\n\n\n\ntidy_book %&gt;%\n  inner_join(get_sentiments(\"bing\")) |&gt; # keeps only words with sentiments in bing\n  count(sentiment, sort = TRUE)\n\n\n\n\n\ntidy_book |&gt;\n  inner_join(get_sentiments(\"afinn\")) |&gt; # get sentiment of each word\n  summarize(sum(value))\n\n\n\n\n\ntidy_book |&gt;\n  inner_join(get_sentiments(\"afinn\")) |&gt; # get sentiment of each word\n  group_by(word) |&gt;\n  summarize(sum_sentiment = sum(value))|&gt; \n  slice_max(sum_sentiment,n=10)\n\n\n\n\n\ntidy_book |&gt;\n  inner_join(get_sentiments(\"afinn\")) |&gt; # get sentiment of each word\n  group_by(word) |&gt;\n  summarize(sum_sentiment = sum(value))|&gt; \n  slice_min(sum_sentiment,n=10)\n\n\n\n\n\nset.seed(1234)\n\ntop_positives &lt;- \n      tidy_book |&gt;\n      inner_join(get_sentiments(\"afinn\")) |&gt; # get sentiment of each word\n      group_by(word) |&gt;\n      summarize(sum_sentiment = sum(value))|&gt; \n      slice_max(sum_sentiment,n=50) \n\nwordcloud(words = top_positives$word, \n        freq = top_positives$sum_sentiment, \n        colors = brewer.pal(5,\"Blues\"),\n        random.order = FALSE)\n\n\n\n\n\ntop_negatives &lt;- \n      tidy_book |&gt;\n      inner_join(get_sentiments(\"afinn\")) |&gt; # get sentiment of each word\n      group_by(word) |&gt;\n      summarize(sum_sentiment = sum(value))|&gt; \n      slice_min(sum_sentiment,n=50) \n\nwordcloud(words = top_negatives$word, \n        freq = abs(top_negatives$sum_sentiment), \n        colors = brewer.pal(5,\"Reds\"),\n        random.order = FALSE)"
  },
  {
    "objectID": "lab-16-tidy-text.html#packages",
    "href": "lab-16-tidy-text.html#packages",
    "title": "Text Analysis in R",
    "section": "",
    "text": "In addition to tidyverse we will be using other packages today"
  },
  {
    "objectID": "lab-16-tidy-text.html#tidytext",
    "href": "lab-16-tidy-text.html#tidytext",
    "title": "Text Analysis in R",
    "section": "",
    "text": "Using tidy data principles can make many text mining tasks easier, more effective, and consistent with tools already in wide use.\nLearn more at https://www.tidytextmining.com/."
  },
  {
    "objectID": "lab-16-tidy-text.html#what-is-tidy-text",
    "href": "lab-16-tidy-text.html#what-is-tidy-text",
    "title": "Text Analysis in R",
    "section": "",
    "text": "Consider some of the lyrics from There Is a Light That Never Goes Out by The Smiths\n\ntext &lt;- c(\"Take me out tonight\",\n          \"Where there's music and there's people\",\n          \"And they're young and alive\",\n          \"Driving in your car\",\n          \"I never never want to go home\",\n          \"Because I haven't got one\",\n          \"Anymore\")\ntext\n\nThis form of the text is not very easy to do analysis with. Think about the data you have been working with this week.\n\n\nQUESTION: How should data be formatted? ##################################\n\n\n\nThis will clean (tidy) the data!\n\ntext_df &lt;- tibble(line = 1:7, text = text)\n\ntext_df\n\nThis is still not quite right. Today, we want a word on each line. This process is called tokenizing, where we separate words from their sentences,.\n\ntext_df |&gt;\n  unnest_tokens(word, text)\n\n\n\n\nI am going to have R read in all of your results from the survey before.\n\n#listening &lt;- read_csv(\"data/listening.csv\")\n\nlistening &lt;- \n  read_sheet(\"https://docs.google.com/spreadsheets/d/1WXKCB_a8AO3CFp49VQ5yTSHYpOfDm1WA9mmYN5jTH1M/edit?usp=sharing\") |&gt; \n  janitor::clean_names() |&gt; # re-names the column to be called \"songs\"\n  select(songs) # keeps only the column called songs\n  \nlistening\n\n\n\n\n\nlistening |&gt;\n  unnest_tokens(word, songs) |&gt;\n  count(word, sort = TRUE)\n\n\n\n\n\nword_counts&lt;-\n  listening |&gt;\n  unnest_tokens(word, songs) |&gt;\n  anti_join(stop_words) |&gt;                           \n  count(word, sort = TRUE)\n\nword_counts\n\n\n\n\n\nword_counts |&gt;\n  ggplot(aes(x = fct_reorder(word, n), y = n)) +\n  geom_col() +\n  labs(x = \"Common words\", y = \"Count\") +\n  coord_flip()\n\n\n\n\n\nset.seed(1234)\nwordcloud(words = top20_songs$word, \n          freq = top20_songs$n, \n          colors = brewer.pal(5,\"Blues\"),\n          random.order = FALSE)\n#Note: You may need to increase the size of your plot area to get this to display properly"
  },
  {
    "objectID": "lab-16-tidy-text.html#text-analysis-on-a-whole-book",
    "href": "lab-16-tidy-text.html#text-analysis-on-a-whole-book",
    "title": "Text Analysis in R",
    "section": "",
    "text": "Let’s look at a whole book, The Road to Oz by L. Frank Baum.\nWe will use an open source website of books called Project Gutenberg.\n\nfull_text &lt;- gutenberg_download(26624) # The number is the ebook number for the book on the website. \n\nNow it’s time to tokenize, where we separate words from their sentences, and tidy were R puts each word on its own line in a dataset.\n\ntidy_book &lt;- full_text |&gt; # sends the full book \n  mutate(line = row_number()) |&gt; # give s row number to each line\n  unnest_tokens(word,text) # separates the words \"tokens\" onto their own lines\n  # This stands for un - nest - tokens\n\ntidy_book\n\n\n\n\ntidy_book |&gt;\n  count(word,sort = T) # this counts and sorts by top counts\n\n\n\n\nNow we want a visualization to compare the common words\n\n  tidy_book |&gt;\n  count(word) |&gt;\n  slice_max(n, n = 20) |&gt;\n  ggplot(aes(n, fct_reorder(word, n))) +  \n    geom_col()\n\n\n\nQUESTION: Why is this not particularly useful?\nDiscuss with your group mates a step we could take before making this graph that would lead to a better, more informative, visualization. ################################## ANSWER:\n\n\n\n\n\n\n\n\nIn computing, stop words are words which are filtered out before or after processing of text.\nThey usually refer to the most common words in a language, but there is not a single list of stop words used by all natural language processing tools.\n\n\n\n\n\nget_stopwords()\n\n\n\n\n\nget_stopwords(language = \"es\")\n\n\n\n\n\n  tidy_book |&gt;\n  anti_join(get_stopwords(source = \"smart\")) |&gt; # This keeps only non-stopwords\n  count(word, sort = TRUE) |&gt; #count the frequency of the remaining words\n  slice_max(n, n = 20) |&gt; # keep only the top 20 counts\n  ggplot(aes(n, fct_reorder(word, n))) +  #start a blank graph\n    geom_col() #add a column (bar) graph to the blank graph\n\n\n\n\nOne way to think about text is the sentiment of words or phrases. These are values that represent positive and negative words and phrases. For example, if someone died, that is more negative than say, if someone got a promotion.\nExplore some sentiment lexicons, or collections of words and phrases with assigned sentiments.\n\nget_sentiments(\"bing\")\n\n\nget_sentiments(\"afinn\")\n\n\n\nQUESTION: How are these two lexicons of sentiment different? ################################## ANSWER:\n\n\n\n\n\n\n\nWe want a sentiment for each word but only for the words in our book.\n\ntidy_book\n\ntidy_book %&gt;%\n  inner_join(get_sentiments(\"bing\")) |&gt; # keeps only words with sentiments in bing\n  count(sentiment, sort = TRUE)\n\n\n\nQUESTION: What does your group think the “inner join” does? ################################## ANSWER:\n\n\n\n\n\n\nQUESTION: Consider the final result. What does this output tell you about the book? ################################## ANSWER:\n\n\n\n\n\n\nQUESTION: This is very broad information, what else do you want to know about the sentiment of the text? How could we change out approach (not code) to get more informative/useful results? ################################## ANSWER:\n\n\n\n\n\n\n\nLet’s consider the afinn lexicon now.\n\ntidy_book |&gt;\n  inner_join(get_sentiments(\"afinn\")) |&gt; # get sentiment of each word\n  summarize(sum(value))\n\n\n\nQUESTION: When we used bing lexicon above we counted the number of positive words and the number of negative words.\nThis new code adds up the value of all the words assigned sentiments.\nFor example, the word happy has a sentiment of 3, and sad has a sentiment of -2, and if we add them, 3 + (-2) we get an overall sentiment of 1.\nCompare and contrast the application of afinn and bing:\n\nWhich gives more information about your book?\nWhich is lexicon is more likely to be subjective? Why?\n\n\n\n\nANSWER:"
  },
  {
    "objectID": "lab-16-tidy-text.html#dive-deeper",
    "href": "lab-16-tidy-text.html#dive-deeper",
    "title": "Text Analysis in R",
    "section": "",
    "text": "The top 10 positive words:\n\ntidy_book |&gt;\n  inner_join(get_sentiments(\"afinn\")) |&gt; # get sentiment of each word\n  group_by(word) |&gt;\n  summarize(sum_sentiment = sum(value))|&gt; \n  slice_max(sum_sentiment,n=10)\n\nThis code\n\nfinds the sentiment for each word (such as happy has a sentiment of 3),\ntells R to sum across each word. If happy was in the text 3 times, it would do 3+3+3 for happy and give it a score of 9.\noutputs the top 10 words that have contributed the most sentiment in the entire book\n\nSimilarly the top 10 most negative words.\n\ntidy_book |&gt;\n  inner_join(get_sentiments(\"afinn\")) |&gt; # get sentiment of each word\n  group_by(word) |&gt;\n  summarize(sum_sentiment = sum(value))|&gt; \n  slice_min(sum_sentiment,n=10)\n\n\n\n\n\nset.seed(1234)\n\ntop_positives &lt;- \n      tidy_book |&gt;\n      inner_join(get_sentiments(\"afinn\")) |&gt; # get sentiment of each word\n      group_by(word) |&gt;\n      summarize(sum_sentiment = sum(value))|&gt; \n      slice_max(sum_sentiment,n=50) \n\nwordcloud(words = top_positives$word, \n        freq = top_positives$sum_sentiment, \n        colors = brewer.pal(5,\"Blues\"),\n        random.order = FALSE)\n\n\n\n\n\ntop_negatives &lt;- \n      tidy_book |&gt;\n      inner_join(get_sentiments(\"afinn\")) |&gt; # get sentiment of each word\n      group_by(word) |&gt;\n      summarize(sum_sentiment = sum(value))|&gt; \n      slice_min(sum_sentiment,n=50) \n\nwordcloud(words = top_negatives$word, \n        freq = abs(top_negatives$sum_sentiment), \n        colors = brewer.pal(5,\"Reds\"),\n        random.order = FALSE)"
  },
  {
    "objectID": "lab-16-tidy-text.html#your-turn",
    "href": "lab-16-tidy-text.html#your-turn",
    "title": "Text Analysis in R",
    "section": "",
    "text": "What book does your table want to analyze today?\n\nGo to https://www.gutenberg.org/browse/scores/top\nClick on a top 100 book or search a book you are interested in. Not all are available and not all will be downloadable.\nReplace 26624 below with your own choice. You need the EBook-No. from the website. It can be found by scrolling down in the table on the page for the book or in the URL.\n\n\nfull_text &lt;- gutenberg_download(26624) # The number is the ebook number for the book on the website. \n\n\n\n\ntidy_book &lt;- full_text |&gt; # sends the full book \n  mutate(line = row_number()) |&gt; # give s row number to each line\n  unnest_tokens(word,text) # separates the words \"tokens\" onto their own lines\n  # This stands for un - nest - tokens\n\ntidy_book\n\n\n\n\n\n  tidy_book |&gt;\n  count(word) |&gt;\n  slice_max(n, n = 20) |&gt;\n  ggplot(aes(n, fct_reorder(word, n))) +  \n    geom_col()\n\n\n\n\n\n  tidy_book |&gt;\n  anti_join(get_stopwords(source = \"smart\")) |&gt; # This keeps only non-stopwords\n  count(word, sort = TRUE) |&gt; #count the frequency of the remaining words\n  slice_max(n, n = 20) |&gt; # keep only the top 20 counts\n  ggplot(aes(n, fct_reorder(word, n))) +  #start a blank graph\n    geom_col() #add a column (bar) graph to the blank graph\n\n\n\n\n\ntidy_book %&gt;%\n  inner_join(get_sentiments(\"bing\")) |&gt; # keeps only words with sentiments in bing\n  count(sentiment, sort = TRUE)\n\n\n\n\n\ntidy_book |&gt;\n  inner_join(get_sentiments(\"afinn\")) |&gt; # get sentiment of each word\n  summarize(sum(value))\n\n\n\n\n\ntidy_book |&gt;\n  inner_join(get_sentiments(\"afinn\")) |&gt; # get sentiment of each word\n  group_by(word) |&gt;\n  summarize(sum_sentiment = sum(value))|&gt; \n  slice_max(sum_sentiment,n=10)\n\n\n\n\n\ntidy_book |&gt;\n  inner_join(get_sentiments(\"afinn\")) |&gt; # get sentiment of each word\n  group_by(word) |&gt;\n  summarize(sum_sentiment = sum(value))|&gt; \n  slice_min(sum_sentiment,n=10)\n\n\n\n\n\nset.seed(1234)\n\ntop_positives &lt;- \n      tidy_book |&gt;\n      inner_join(get_sentiments(\"afinn\")) |&gt; # get sentiment of each word\n      group_by(word) |&gt;\n      summarize(sum_sentiment = sum(value))|&gt; \n      slice_max(sum_sentiment,n=50) \n\nwordcloud(words = top_positives$word, \n        freq = top_positives$sum_sentiment, \n        colors = brewer.pal(5,\"Blues\"),\n        random.order = FALSE)\n\n\n\n\n\ntop_negatives &lt;- \n      tidy_book |&gt;\n      inner_join(get_sentiments(\"afinn\")) |&gt; # get sentiment of each word\n      group_by(word) |&gt;\n      summarize(sum_sentiment = sum(value))|&gt; \n      slice_min(sum_sentiment,n=50) \n\nwordcloud(words = top_negatives$word, \n        freq = abs(top_negatives$sum_sentiment), \n        colors = brewer.pal(5,\"Reds\"),\n        random.order = FALSE)"
  }
]